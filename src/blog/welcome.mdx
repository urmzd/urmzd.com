---
title: "Hey, I'm Urmzd!"
description: "An introduction to a curious mind exploring history, martial arts, technology, philosophy, and the amazing creations of humanity."
pubDate: 2025-01-31
tags: ["introduction", "personal"]
---

import PreviewLink from '../components/PreviewLink';
import WelcomeTimeline from '../components/WelcomeTimeline';
import ScriptInline from '../components/ScriptInline';
import Phonetic from '../components/Phonetic';
import ExploreCard from '../components/ExploreCard';

Hey! I'm Urmzd Mukhammadnaim (<Phonetic client:load ipa="ʊərˈmuːzd mʊˌhɑːmɑdˈnaɪm" />), and welcome to **urmzd.com**!

My father — a history enthusiast from <PreviewLink client:load href="https://en.wikipedia.org/wiki/Badakhshan_Province">Badakhshan</PreviewLink>, Afghanistan, believed to be the homeland of the prophet <PreviewLink client:load href="https://en.wikipedia.org/wiki/Zoroaster">Zoroaster</PreviewLink> — gave me a name steeped in ancient meaning. *Urmzd* derives from "Ahura Mazda" (<ScriptInline client:load letters="AHURAMAZDA" targetScript="arabic" />), the supreme deity of <PreviewLink client:load href="https://en.wikipedia.org/wiki/Zoroastrianism">Zoroastrianism</PreviewLink>, humanity's oldest monotheistic faith. The name combines the <PreviewLink client:load href="https://en.wikipedia.org/wiki/Proto-Indo-Iranian_language">Proto-Indo-Iranian</PreviewLink> words *Ahura* ("Lord") and *Mazda* ("Wisdom") — together meaning "Lord of Wisdom." The deeper etymology reveals even more: *Ahura* traces to Proto-Indo-European roots meaning "to engender, beget," while *Mazda* comes from "placing one's mind" — to give rise to one's mind, or as I hold it, to shape and share knowledge.

This name holds great meaning to me, and this site is my attempt to stay true to it.

This is meant to be a living document — posts may be refined, opinions may evolve, and ideas may deepen as I grow. Since I value transparency and honesty, all changes are tracked on <PreviewLink client:load href="https://github.com/urmzd/urmzd.com">github.com/urmzd/urmzd.com</PreviewLink>, where you can see exactly what evolved and when. There, you can explore, analyze, and critique it to your heart's desire.
You can open up issues if you want to request a specific topic be covered. Alternatively, if you aren't in tech like the vast majority of people,
just shoot an email to [hello@urmzd.com](mailto:hello@urmzd.com). I commit to responding as reasonably soon as I can. For now, I plan on publishing one written piece a week, but more often if time allows for it!

A small note before we continue: each post will close with a brief snippet from a different domain. I hope that by sharing these, it'll encourage people to explore topics beyond what they know or believe they know.

What does staying true to that name look like in practice? It starts with three principles that guide everything I do.

## Core Principles

I hold *honesty*, *freedom*, and *family* dear to me. These core principles inform the decisions I make.

- Honesty: *The pursuit and belief in absolute truth within oneself.*
- Freedom: *A state in which one is unbounded by the assumptions created internally or externally*
- Family: *Relationships in which the kindness received produces profound changes to the state of one's life*

With this in mind, I believe it's difficult to learn about someone without understanding their story. Instead of just describing who I am, let me show you through the milestones that shaped me.

## The Timeline

<WelcomeTimeline client:load />

That's the path so far. Now, as promised, here's something to spark curiosity in a completely different direction.

## Snippet of the Week

With the rapid development and integration of LLMs, I believe it's important to understand the foundations that brought us here — the people, the discoveries, and how those changes shaped where we are today.

<ExploreCard client:load title="Cosine Similarity — The Math Behind AI Search">

### The Foundation

To learn more about the people, take a look here:

- <PreviewLink client:load href="https://en.wikipedia.org/wiki/History_of_trigonometry">History of Trigonometry</PreviewLink>

To learn more about the math, take a look here:

- <PreviewLink client:load href="https://en.wikipedia.org/wiki/Dot_product">Dot Product</PreviewLink>
- <PreviewLink client:load href="https://en.wikipedia.org/wiki/Cosine_similarity">Cosine Similarity</PreviewLink>

### The Math

**Vector** — An ordered list of numbers representing a point or direction in n-dimensional space:

$$
\vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} \in \mathbb{R}^n
$$

**Dot Product** — The sum of element-wise products of two vectors:

$$
\vec{a} \cdot \vec{b} = \sum_{i=1}^{n} a_i \cdot b_i
$$

**Norm (Euclidean)** — The "length" or magnitude of a vector:

$$
\lVert\vec{v}\rVert = \sqrt{\sum_{i=1}^{n} v_i^2}
$$

**Cosine Similarity** — Measures directional similarity between two vectors (ranges from -1 to 1):

$$
\text{sim}(\vec{a}, \vec{b}) = \frac{\vec{a} \cdot \vec{b}}{\lVert\vec{a}\rVert \cdot \lVert\vec{b}\rVert}
$$

### The Code

```python
from typing import TypeAlias
from math import sqrt

# A Vector is an N-dimensional point in space.
Vector: TypeAlias = list[float]

# An Embedding is a Vector that encodes semantic meaning.
Embedding: TypeAlias = Vector

# Dot product: sum of element-wise products of two vectors.
def dot_product(a_vec: Vector, b_vec: Vector) -> float:
    return sum(a*b for a,b in zip(a_vec, b_vec))

# Euclidean norm: the magnitude (length) of a vector.
def norm(vec: Vector) -> float:
    return sqrt(sum(a**2 for a in vec))

# Cosine similarity: measures directional alignment between two vectors (-1 to 1).
def cosine_similarity(a: Embedding, b: Embedding) -> float:
    return dot_product(a, b) / (norm(a) * norm(b))
```

### The Connection

Modern LLMs like GPT, Claude, Gemini, and others convert text into high-dimensional vectors called **embeddings** — numerical representations that capture semantic meaning. Words, sentences, or entire documents that are similar in meaning end up as vectors pointing in similar directions.

When you search for something using an AI-powered tool, or when a chatbot retrieves relevant context from a knowledge base, **cosine similarity** is often the mechanism comparing your query's embedding against stored embeddings. This is the foundation of:

- **Semantic search**: Finding documents by meaning, not just keyword matches
- **Retrieval-Augmented Generation (RAG)**: Giving LLMs relevant context before answering
- **Recommendation systems**: Suggesting similar content based on vector proximity

The math above — developed centuries ago by mathematicians studying triangles and angles — now powers the similarity calculations running billions of times daily across AI systems worldwide.

</ExploreCard>

## Recommended Books

Two books that have shaped my thinking:

- <PreviewLink client:load href="https://www.amazon.com/Meditations-Penguin-Classics-Marcus-Aurelius/dp/0140449337">Meditations: Marcus Aurelius</PreviewLink>
- <PreviewLink client:load href="https://www.amazon.com/Four-Thousand-Weeks-Management-Mortals/dp/0374159122">Four Thousand Weeks: Oliver Burkeman</PreviewLink>
